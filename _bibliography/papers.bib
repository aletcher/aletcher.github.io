---
---

@article{pauli-propagation,
  year = {2025},
  title = {How Do Pauli Gates Propagate?},
  author = {Letcher, A.},
  abbr = {preprint},
  abstract = {Among the primary building blocks of quantum circuits are the Pauli gates (X, Y, Z) and their controlled counterparts (usually CX or CZ). The latter are commonly stacked together to form entanglement layers in a linear / reverse-linear / pairwise / full or circular fashion, as implemented in the quantum computing library, Qiskit. This short article addresses the simple question of how Pauli gates ‘propagate’ across any number m of such layers. While this question is easily resolved for a single layer (m = 1), we prove explicit commutation relations for arbitrary m ∈ N using elementary combinatorics.}
}

@article{ued,
  title={An Optimisation Framework for Unsupervised Environment Design},
  author={N. Monette and A. Letcher and M. Beukman and M. Jackson and A. Rutherford and A. Goldie and J. Foerster},
  year={2025},
  abbr={RLC},
  html={https://openreview.net/pdf?id=WnknYUybWX},
  journal={Reinforcement Learning Conference},
  abstract={For reinforcement learning agents to be deployed in high-risk settings, they must achieve a high level of robustness to unfamiliar scenarios. One method for improving robustness is unsupervised environment design (UED), a suite of methods aiming to maximise an agent's generalisability across configurations of an environment. In this work, we study UED from an optimisation perspective, providing stronger theoretical guarantees for practical settings than prior work. Whereas previous methods relied on guarantees if they reach convergence, our framework employs a nonconvex-strongly-concave objective for which we provide a provably convergent algorithm in the zero-sum setting. We empirically verify the efficacy of our method, outperforming prior methods in a number of environments with varying difficulties.}
}

@article{polymatrix,
  title={Polymatrix Competitive Gradient Descent},
  author={J. Ma and A. Letcher and F. Sch{\"a}fer and Y. Shi and A. Anandkumar},
  year={2030},
  abbr={preprint},
  arxiv={2111.08565},
  journal={preprint},
  abstract={Many economic games and machine learning approaches can be cast as competitive optimization problems where multiple agents are minimizing their respective objective function, which depends on all agents' actions. While gradient descent is a reliable basic workhorse for single-agent optimization, it often leads to oscillation in competitive optimization. In this work we propose polymatrix competitive gradient descent (PCGD) as a method for solving general sum competitive optimization involving arbitrary numbers of agents. The updates of our method are obtained as the Nash equilibria of a local polymatrix approximation with a quadratic regularization, and can be computed efficiently by solving a linear system of equations. We prove local convergence of PCGD to stable fixed points for n-player general-sum games, and show that it does not require adapting the step size to the strength of the player-interactions. We use PCGD to optimize policies in multi-agent reinforcement learning and demonstrate its advantages in Snake, Markov soccer and an electricity market game. Agents trained by PCGD outperform agents trained with simultaneous gradient descent, symplectic gradient adjustment, and extragradient in Snake and Markov soccer games and on the electricity market game, PCGD trains faster than both simultaneous gradient descent and the extragradient method.}
}

@article{quantum-gradient-bounds,
  doi = {10.22331/q-2024-09-25-1484},
  title = {Tight and {E}fficient {G}radient {B}ounds for {P}arameterized {Q}uantum {C}ircuits},
  author = {Letcher, A. and Woerner, S. and Zoufal, C.},
  journal = {{Quantum Journal}},
  abbr = {QJ},
  html = {https://quantum-journal.org/papers/q-2024-09-25-1484/},
  volume = {8},
  pages = {1484},
  year = {2024},
  arxiv = {2309.12681},
  abstract = {The training of a parameterized model largely depends on the landscape of the underlying loss function. In particular, vanishing gradients are a central bottleneck in the scalability of variational quantum algorithms (VQAs), and are known to arise in various ways. However, a caveat of most existing gradient bound results is the requirement of t-design circuit assumptions that are typically not satisfied in practice. In this work, we loosen these assumptions altogether and derive tight upper and lower bounds on loss and gradient concentration for a large class of parameterized quantum circuits and arbitrary observables, which are significantly stronger than prior work. Moreover, we show that these bounds, as well as the variance of the loss itself, can be estimated efficiently and classically-providing practical tools to study the loss landscapes of VQA models, including verifying whether or not a circuit/observable induces barren plateaus. In particular, our results can readily be leveraged to rule out barren plateaus for a realistic class of ansätze and mixed observables, namely, observables containing a non-vanishing local term. This insight has direct implications for hybrid Quantum Generative Adversarial Networks (qGANs). We prove that designing the discriminator appropriately leads to 1-local weights that stay constant in the number of qubits, regardless of discriminator depth. This implies that qGANs with appropriately chosen generators do not suffer from barren plateaus even at scale-making them a promising candidate for applications in generative quantum machine learning. We demonstrate this result by training a qGAN to learn a 2D mixture of Gaussian distributions with up to 16 qubits, and provide numerical evidence that global contributions to the gradient, while initially exponentially small, may kick in substantially over the course of training.}
}

@article{adversarial,
  title={Adversarial Cheap Talk},
  author={C. Lu and T. Willi and A. Letcher and J. Foerster},
  year={2023},
  html={https://openreview.net/forum?id=3wxUzU2ZnM},
  abbr={ICML},
  arxiv={2211.11030},
  journal={International Conference on Machine Learning},
  abstract={Adversarial attacks in reinforcement learning (RL) often assume highly-privileged access to the learning agent’s parameters, environment or data. Instead, this paper proposes a novel adversarial setting called a Cheap Talk MDP in which an Adversary has a minimal range of influence over the Victim. Parameterised as a deterministic policy that only conditions on the current state, an Adversary can merely append information to a Victim’s observation. To motivate the minimum-viability, we prove that in this setting the Adversary cannot occlude the ground truth, influence the underlying dynamics of the environment, introduce non-stationarity, add stochasticity, see the Victim’s actions, or access their parameters. Additionally, we present a novel meta-learning algorithm to train the Adversary, called adversarial cheap talk (ACT). Using ACT, we demonstrate that the resulting Adversary still manages to influence the Victim’s training and test performance despite these restrictive assumptions. Affecting train-time performance reveals a new attack vector and provides insight into the success and failure modes of existing RL algorithms. More specifically, we show that an ACT Adversary is capable of harming performance by interfering with the learner’s function approximation and helping the Victim’s performance by appending useful features. Finally, we demonstrate that an ACT Adversary can append information during train-time to directly and arbitrarily control the Victim at test-time in a zero-shot manner.}
}

@article{dpo,
  title={Discovered Policy Optimisation},
  author={C. Lu and J. Kuba and A. Letcher and L. Metz and C. Schroeder de Witt and J. Foerster},
  journal={Advances in Neural Information Processing Systems},
  html={https://openreview.net/forum?id=vsQ4gufZ-Ve},
  arxiv={2210.05639},
  year={2022},
  abbr={NIPS},
  abstract={Tremendous progress has been made in reinforcement learning (RL) over the past decade. Most of these advancements came through the continual development of new algorithms, which were designed using a combination of mathematical derivations, intuitions, and experimentation. Such an approach of creating algorithms manually is limited by human understanding and ingenuity. In contrast, meta-learning provides a toolkit for automatic machine learning method optimisation, potentially addressing this flaw. However, black-box approaches which attempt to discover RL algorithms with minimal prior structure have thus far not outperformed existing hand-crafted algorithms. Mirror Learning, which includes RL algorithms, such as PPO, offers a potential middle-ground starting point: while every method in this framework comes with theoretical guarantees, components that differentiate them are subject to design. In this paper we explore the Mirror Learning space by meta-learning a "drift" function. We refer to the immediate result as Learnt Policy Optimisation (LPO). By analysing LPO we gain original insights into policy optimisation which we use to formulate a novel, closed-form RL algorithm, Discovered Policy Optimisation (DPO). Our experiments in Brax environments confirm state-of-the-art performance of LPO and DPO, as well as their transfer to unseen settings.}
}

@article{cola,
  title={COLA: Consistent Learning with Opponent-Learning Awareness},
  author={T. Willi and J. Treutlein and A. Letcher and J. Foerster},
  journal={International Conference on Machine Learning},
  year={2022},
  abbr={ICML},
  html={https://proceedings.mlr.press/v162/willi22a/willi22a.pdf},
  arxiv={2203.04098},
  abstract={Learning in general-sum games can be unstable and often leads to socially undesirable, Pareto-dominated outcomes. To mitigate this, Learning with Opponent-Learning Awareness (LOLA) introduced opponent shaping to this setting, by accounting for the agent's influence on the anticipated learning steps of other agents. However, the original LOLA formulation (and follow-up work) is inconsistent because LOLA models other agents as naive learners rather than LOLA agents. In previous work, this inconsistency was suggested as a cause of LOLA's failure to preserve stable fixed points (SFPs). First, we formalize consistency and show that higher-order LOLA (HOLA) solves LOLA's inconsistency problem if it converges. Second, we correct a claim made in the literature, by proving that, contrary to Schäfer and Anandkumar (2019), Competitive Gradient Descent (CGD) does not recover HOLA as a series expansion. Hence, CGD also does not solve the consistency problem. Third, we propose a new method called Consistent LOLA (COLA), which learns update functions that are consistent under mutual opponent shaping. It requires no more than second-order derivatives and learns consistent update functions even when HOLA fails to converge. However, we also prove that even consistent update functions do not preserve SFPs, contradicting the hypothesis that this shortcoming is caused by LOLA's inconsistency. Finally, in an empirical evaluation on a set of general-sum games, we find that COLA finds prosocial solutions and that it converges under a wider range of learning rates than HOLA and LOLA. We support the latter finding with a theoretical result for a simple game.}
}

@article{global,
  title={On the Impossibility of Global Convergence in Multi-Loss Optimization},
  author={A. Letcher},
  journal={International Conference on Learning Representations},
  year={2021},
  abbr={ICLR},
  code={https://github.com/aletcher/impossibility-global-convergence},
  html={https://openreview.net/pdf?id=NQbnPjPYaG6},
  abstract={Under mild regularity conditions, gradient-based methods converge globally to a critical point in the single-loss setting. This is known to break down for vanilla gradient descent when moving to multi-loss optimization, but can we hope to build some algorithm with global guarantees? We negatively resolve this open problem by proving that any reasonable algorithm will exhibit limit cycles or diverge to infinite losses in some differentiable game, even in two-player games with zero-sum interactions. A reasonable algorithm is simply one which avoids strict maxima, an exceedingly weak assumption since converging to maxima would be the opposite of minimization. This impossibility theorem holds even if we impose existence of a strict minimum and no other critical points. The proof is constructive, enabling us to display explicit limit cycles for existing gradient-based methods. Nonetheless, it remains an open question whether cycles arise in high-dimensional games of interest to ML practitioners, such as GANs or multi-agent RL.},
  arxiv={2005.12649}
}

@article{ridge,
  title={Ridge Rider: Finding Diverse Solutions by Following Eigenvectors of the Hessian},
  author={J. Parker-Holder and L. Metz and C. Resnick and H. Hu and A. Lerer and A. Letcher and A. Peysakhovich and A. Pacchiano and J. Foerster},
  abbr={NIPS},
  journal={Advances in Neural Information Processing Systems},
  year={2020},
  abstract={Over the last decade, a single algorithm has changed many facets of our lives - Stochastic Gradient Descent (SGD). In the era of ever decreasing loss functions, SGD and its various offspring have become the go-to optimization tool in machine learning and are a key component of the success of deep neural networks (DNNs). While SGD is guaranteed to converge to a local optimum (under loose assumptions), in some cases it may matter which local optimum is found, and this is often context-dependent. Examples frequently arise in machine learning, from shape-versus-texture-features to ensemble methods and zero-shot coordination. In these settings, there are desired solutions which SGD on 'standard' loss functions will not find, since it instead converges to the 'easy' solutions. In this paper, we present a different approach. Rather than following the gradient, which corresponds to a locally greedy direction, we instead follow the eigenvectors of the Hessian, which we call "ridges". By iteratively following and branching amongst the ridges, we effectively span the loss surface to find qualitatively different solutions. We show both theoretically and experimentally that our method, called Ridge Rider (RR), offers a promising direction for a variety of challenging problems.},
  html={https://papers.nips.cc/paper/2020/hash/08425b881bcde94a383cd258cea331be-Abstract.html},
  arxiv={2011.06505},
  blog={https://bair.berkeley.edu/blog/2020/11/13/ridge-rider/}
}

@article{game,
  title={Differentiable Game Mechanics},
  author={A. Letcher and D. Balduzzi and S. Racani{\`e}re and J. Martens and J. Foerster and K. Tuyls and T. Graepel},
  abbr={JMLR},
  journal={Journal of Machine Learning Research},
  year={2019},
  abstract={Deep learning is built on the foundational guarantee that gradient descent on an objective function converges to local minima. Unfortunately, this guarantee fails in settings, such as generative adversarial nets, that exhibit multiple interacting losses. The behavior of gradient-based methods in games is not well understood -- and is becoming increasingly important as adversarial and multi-objective architectures proliferate. In this paper, we develop new tools to understand and control the dynamics in n-player differentiable games. The key result is to decompose the game Jacobian into two components. The first, symmetric component, is related to potential games, which reduce to gradient descent on an implicit function. The second, antisymmetric component, relates to Hamiltonian games, a new class of games that obey a conservation law akin to conservation laws in classical mechanical systems. The decomposition motivates Symplectic Gradient Adjustment (SGA), a new algorithm for finding stable fixed points in differentiable games. Basic experiments show SGA is competitive with recently proposed algorithms for finding stable fixed points in GANs -- while at the same time being applicable to, and having guarantees in, much more general cases.},
  html={http://www.jmlr.org/papers/v20/19-008.html},
  code={https://github.com/deepmind/symplectic-gradient-adjustment},
  arxiv={1905.04926}
}

@article{sos,
  title={Stable Opponent Shaping in Differentiable Games},
  author={A. Letcher and J. Foerster and D. Balduzzi and T. Rockt{\"a}schel and S. Whiteson},
  journal={International Conference on Learning Representations},
  arxiv={1811.08469},
  abbr={ICLR},
  html={https://openreview.net/forum?id=SyGjjsC5tQ},
  code={https://github.com/aletcher/stable-opponent-shaping},
  abstract={A growing number of learning methods are actually games which optimise multiple, interdependent objectives in parallel -- from GANs and intrinsic curiosity to multi-agent RL. Opponent shaping is a powerful approach to improve learning dynamics in such games, accounting for the fact that the 'environment' includes agents adapting to one another's updates. Learning with Opponent-Learning Awareness (LOLA) is a recent algorithm which exploits this dynamic response and encourages cooperation in settings like the Iterated Prisoner's Dilemma. Although experimentally successful, we show that LOLA can exhibit 'arrogant' behaviour directly at odds with convergence. In fact, remarkably few algorithms have theoretical guarantees applying across all differentiable games. In this paper we present Stable Opponent Shaping (SOS), a new method that interpolates between LOLA and a stable variant named LookAhead. We prove that LookAhead locally converges and avoids strict saddles in all differentiable games, the strongest results in the field so far. SOS inherits these desirable guarantees, while also shaping the learning of opponents and consistently either matching or outperforming LOLA experimentally.},
  year={2019}
}

@article{bodyworn,
  author={A. Letcher and J. Trišović and C. Cademartori and X. Chen and J. Xu},
  journal={IEEE International Conference on Acoustics, Speech and Signal Processing},
  title={Automatic Conflict Detection in Police Body-Worn Audio},
  year={2018},
  abbr={ICASSP},
  arxiv={1711.05355},
  html={https://ieeexplore.ieee.org/document/8461425},
  abstract={Automatic conflict detection has grown in relevance with the advent of body-worn technology, but existing metrics such as turn-taking and overlap are poor indicators of conflict in police-public interactions. Moreover, standard techniques to compute them fall short when applied to such diversified and noisy contexts. We develop a pipeline catered to this task combining adaptive noise removal, non-speech filtering and new measures of conflict based on the repetition and intensity of phrases in speech. We demonstrate the effectiveness of our approach on body-worn audio data collected by the Los Angeles Police Department.}
}
